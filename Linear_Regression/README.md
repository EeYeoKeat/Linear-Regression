# Mathematical Concept in Linear Regression

Linear regression is an approach to modelling the relationship between two variables by fitting a linear equation to the data. It also known as one of the supervised learning algorithm in machine learning that commonly used in regression analysis.

**Content covered in this folder**
1. [Solving the equation of simple Linear Regression using Gradient Descent](https://github.com/ee2110/Machine_Learning_Mathematics/blob/master/Linear_Regression/Gradient_Descent_in_Linear_Regression.ipynb)

Gradient descent is an optimization algorithm used to find the values of the parameters in a function that iteratively move towards minimizing the cost function. It is iterative algoritim to update the parameters of the model which is commonly called coefficients in Linear Regression.

2. [Compare the optimization method between using Gradient Descent and the Normal Equation](https://github.com/ee2110/Machine_Learning_Mathematics/blob/master/Linear_Regression/Univariate_Linear_Regression.ipynb)

The simple linear regression can apply Normal Equation to find the closest solution of a linear equation. Normal Equation is an analytical approach that using Least Square Cost Function method to discover the best-fit line for the set of paired data.


References:
1. [Notes](https://www.amherst.edu/system/files/media/1287/SLR_Leastsquares.pdf) about Least Squares in SLR from [Amherst College](https://www.amherst.edu/).  Alternative copy of note [here](https://github.com/ee2110/Machine_Learning_Mathematics/blob/master/Linear_Regression/SLR_Leastsquares-notes_from_Amherst_College.pdf).
2. Useful explanation video from [Jazon Jiao's YouTube Channel](https://www.youtube.com/channel/UC8bvGUlaTahF0lHhMsZFhfw).